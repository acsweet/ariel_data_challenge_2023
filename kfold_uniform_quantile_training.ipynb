{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2123e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import sys, os, time\n",
    "import pickle\n",
    "import h5py\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow_addons.optimizers import CyclicalLearningRate\n",
    "\n",
    "from utils import get_tf_datasets, fetch_callbacks, get_tf_dataset_sampled, quantile_loss_wrapper\n",
    "from model import get_model\n",
    "\n",
    "# from competition baseline\n",
    "from base_code.helper import to_observed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f75edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858f827a",
   "metadata": {},
   "source": [
    "# Load data and declare variables\n",
    "- e.g. file paths, targets, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6c2a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust as needed\n",
    "training_path = 'data/FullDataset/TrainingData'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0e0599",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_GT_path = os.path.join(training_path, 'Ground Truth Package')\n",
    "GT_trace_path = os.path.join(training_GT_path, 'Tracedata.hdf5')\n",
    "\n",
    "spectral_training_data = h5py.File(os.path.join(training_path,'SpectralData.hdf5'),\"r\")\n",
    "aux_data = pd.read_csv(os.path.join(training_path,'AuxillaryTable.csv'))\n",
    "soft_label_data = pd.read_csv(os.path.join(training_GT_path, 'FM_Parameter_Table.csv'))\n",
    "\n",
    "trace_file = h5py.File(GT_trace_path)\n",
    "\n",
    "if 'Unnamed: 0' in soft_label_data.columns:\n",
    "    soft_label_data = soft_label_data.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "# assume this order?\n",
    "target_labels = ['planet_radius','planet_temp','log_H2O','log_CO2','log_CO','log_CH4','log_NH3']\n",
    "num_targets = len(target_labels)\n",
    "\n",
    "target_scales_dict = {\n",
    "    'planet_radius': [0.1, 3],\n",
    "    'planet_temp': [0, 7000],\n",
    "    'log_H2O': [-12, -1],\n",
    "    'log_CO2': [-12, -1],\n",
    "    'log_CO': [-12, -1],\n",
    "    'log_CH4': [-12, -1],\n",
    "    'log_NH3': [-12, -1]\n",
    "}\n",
    "\n",
    "target_scales_arr = np.array([target_scales_dict[l] for l in target_labels]) # safer, in case dict unordered\n",
    "min_vals = target_scales_arr[:, 0]\n",
    "max_vals = target_scales_arr[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c0b1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_matrix_file = 'spec_matrix.npy'\n",
    "if spec_matrix_file in os.listdir('data/'):\n",
    "    print('opening existing spec matrix file')\n",
    "    with open('data/spec_matrix.npy', 'rb') as f:\n",
    "        spec_matrix = np.load(f)\n",
    "    print('spectra shape:', spec_matrix.shape)\n",
    "else:\n",
    "    print('constructing spec matrix and saving to file')\n",
    "    start_time = time.time()\n",
    "    spec_matrix = to_observed_matrix(spectral_training_data, aux_data)\n",
    "    with open('data/spec_matrix.npy', 'wb') as f:\n",
    "        np.save(f, spec_matrix)\n",
    "    print(\"finished after: {}, spectral matrix shape: {}\".format(time.time() - start_time, spec_matrix.shape))\n",
    "\n",
    "noise = spec_matrix[:, :, 2]\n",
    "spectra = spec_matrix[:, :, 1]\n",
    "wl_grid = spec_matrix[:, :, 0]\n",
    "bin_width = spec_matrix[:, :, 3]\n",
    "\n",
    "global_spectra_mean = np.mean(spectra)\n",
    "global_spectra_std = np.std(spectra)\n",
    "print(f'spectra mean: {global_spectra_mean}, std: {global_spectra_std}')\n",
    "\n",
    "wl_channels = spectra.shape[1] # wavelength_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a251b89c",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "- spectra stats\n",
    "- ratio of planet mass to star mass\n",
    "- star density\n",
    "- planet_semimajor_axis: \n",
    "    - https://en.wikipedia.org/wiki/Orbital_period#Two_bodies_orbiting_each_other\n",
    "- planet equilibrium temperature (planet_eqlbm_temp)\n",
    "    - see equation (7): https://www.astro.princeton.edu/~strauss/FRS113/writeup3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883be2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra_rw_mean = spectra.mean(axis=1)\n",
    "spectra_rw_std = spectra.std(axis=1)\n",
    "spectra_rw_min = spectra.min(axis=1)\n",
    "spectra_rw_max = spectra.max(axis=1)\n",
    "\n",
    "aux_data['spectra_rw_mean'] = spectra_rw_mean\n",
    "aux_data['spectra_rw_std'] = spectra_rw_std\n",
    "aux_data['spectra_rw_min'] = spectra_rw_min\n",
    "aux_data['spectra_rw_max'] = spectra_rw_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0403997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_data['planet_mass_kg/star_mass_kg'] = aux_data['planet_mass_kg']/aux_data['star_mass_kg']\n",
    "aux_data['star_density'] = aux_data['star_mass_kg']*(4/3)*np.pi*(aux_data['star_radius_m']**3)\n",
    "\n",
    "# technically this needs Albedo's constant. i.e. (1 - A)^(1/4)\n",
    "aux_data['planet_eqlbm_temp'] = np.sqrt(aux_data['star_radius_m']/(2*aux_data['planet_distance']))*aux_data['star_temperature']\n",
    "\n",
    "# Kepler's third law (technically this needs the graviational constant)\n",
    "aux_data['planet_semimajor_axis'] = ((aux_data['star_mass_kg'] + aux_data['planet_mass_kg'])*((aux_data['planet_orbital_period']/(2*np.pi))**2))**(1/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a52904",
   "metadata": {},
   "source": [
    "### Plot of dstributions shows skew (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656293f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [c for c in aux_data.columns if c != 'planet_ID']\n",
    "\n",
    "num_cols = 2\n",
    "num_rows = int(np.ceil(len(cols)/num_cols))\n",
    "\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(11, 4*num_rows))\n",
    "\n",
    "for n in range(num_rows):\n",
    "    for m in range(num_cols):\n",
    "        col_idx = n*num_cols + m\n",
    "        sns.histplot(aux_data[cols[col_idx]], kde=False, ax=axs[n, m], label=cols[col_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64f56a0",
   "metadata": {},
   "source": [
    "### Apply log transfrom\n",
    "- natural log, from <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.log.html\">numpy.log</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc42f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [c for c in aux_data.columns if c != 'planet_ID']\n",
    "for c in cols:\n",
    "    aux_data[c] = np.log(aux_data[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208d092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [c for c in aux_data.columns if c != 'planet_ID']\n",
    "\n",
    "num_cols = 2\n",
    "num_rows = int(np.ceil(len(cols)/num_cols))\n",
    "\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(11, 4*num_rows))\n",
    "\n",
    "for n in range(num_rows):\n",
    "    for m in range(num_cols):\n",
    "        col_idx = n*num_cols + m\n",
    "        sns.histplot(aux_data[cols[col_idx]], kde=False, ax=axs[n, m], label=cols[col_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9d6ee1",
   "metadata": {},
   "source": [
    "# Get which planets have trace data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05e72b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "planet_desc_filename = 'planets_data_desc.csv'\n",
    "\n",
    "if planet_desc_filename in os.listdir():\n",
    "    print('reading from file')\n",
    "    df_planet_has_data = pd.read_csv('planets_data_desc.csv')\n",
    "\n",
    "else:\n",
    "    print('building planet_data_desc')\n",
    "    planet_list = [p for p in trace_file.keys()] \n",
    "\n",
    "    planet_data_existence = []\n",
    "    for idx, pl in enumerate(planet_list):\n",
    "        # print(trace_file[pl]['weights'].shape != ())\n",
    "        has_data = trace_file[pl]['weights'].shape != ()\n",
    "        planet_data_existence.append((pl, has_data))\n",
    "        \n",
    "    print(f'finished, total planets: {len(planet_data_existence)}')\n",
    "    \n",
    "    # convenience stuff\n",
    "    df_planet_has_data = pd.DataFrame(planet_data_existence, columns=['planet_id_str', 'has_trace_data'])\n",
    "    df_planet_has_data['planet_id'] = df_planet_has_data['planet_id_str'].str \\\n",
    "                                                                         .replace('Planet_train', '') \\\n",
    "                                                                         .astype(int)\n",
    "    df_planet_has_data['planet_ID'] = df_planet_has_data['planet_id_str'].str.replace('Planet_', '')\n",
    "    df_planet_has_data = df_planet_has_data.sort_values(by=['planet_id']).reset_index(drop=True)\n",
    "    \n",
    "    df_planet_has_data.to_csv(planet_desc_filename, index=False)\n",
    "    \n",
    "total_with_data = df_planet_has_data['has_trace_data'].sum()\n",
    "print(f'has_data: {total_with_data}, no_data: {df_planet_has_data.shape[0] - total_with_data}')\n",
    "df_planet_has_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458f65dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_d_has_trace = soft_label_data.merge(df_planet_has_data, on='planet_ID')\n",
    "soft_d_has_trace.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eddb6ec",
   "metadata": {},
   "source": [
    "# Split out test set\n",
    "- 10% of planets with trace data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2971b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_for_split = ['planet_ID', 'planet_temp', 'planet_radius']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e667c560",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(soft_d_has_trace, \n",
    "                train_size=0.9,\n",
    "                random_state=42)\n",
    "print(f'train_shape: {train.shape}, val_shape: {test.shape}')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f138d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_test_with_trace = test['has_trace_data'].sum()/soft_d_has_trace['has_trace_data'].sum()\n",
    "print(f'percent of test set that has trace data: {percent_test_with_trace:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a914d665",
   "metadata": {},
   "source": [
    "# Kfold training - Phase 1\n",
    "- train on soft targets\n",
    "- loss function of mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e113682",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_columns = aux_data.columns\n",
    "num_splits = 5\n",
    "batch_size = 1024\n",
    "train_repeat_count = 10\n",
    "val_repeat_count = 5\n",
    "\n",
    "# model parameters\n",
    "n_feature_cols = None\n",
    "num_added_features = len(aux_columns) - 1\n",
    "num_spectra_features = wl_channels\n",
    "loss = 'mse'\n",
    "lr = 1e-3\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49585db",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
    "fold = 0\n",
    "scalers = []\n",
    "\n",
    "df_full_history = pd.DataFrame()\n",
    "start_time = time.time()\n",
    "for train_idx, val_idx in kf.split(soft_d_has_trace[cols_for_split], y=None, groups=None):\n",
    "    fold_start_time = time.time()\n",
    "    print('starting fold: {}'.format(fold))\n",
    "    \n",
    "    # get data\n",
    "    train_aux_data = aux_data[aux_columns].iloc[train_idx]\n",
    "    train_targets = soft_label_data.iloc[train_idx]\n",
    "    train_spectra = spectra[train_idx]\n",
    "    train_noise = noise[train_idx]\n",
    "    \n",
    "    val_aux_data = aux_data[aux_columns].iloc[val_idx]\n",
    "    val_targets = soft_label_data.iloc[val_idx]\n",
    "    val_spectra = spectra[val_idx]\n",
    "    val_noise = noise[val_idx]\n",
    "    \n",
    "    # standardize aux_data and targets\n",
    "    aux_data_ss = StandardScaler()\n",
    "    std_train_aux_data = aux_data_ss.fit_transform(train_aux_data.drop(['planet_ID'], axis=1).values)\n",
    "    std_train_targets = (train_targets.drop(['planet_ID'], axis=1)[target_labels].values - min_vals)/(max_vals - min_vals)\n",
    "\n",
    "    std_val_aux_data = aux_data_ss.transform(val_aux_data.drop(['planet_ID'], axis=1).values)\n",
    "    std_val_targets = (val_targets.drop(['planet_ID'], axis=1)[target_labels].values - min_vals)/(max_vals - min_vals)\n",
    "    scalers.append(aux_data_ss)\n",
    "    \n",
    "    # get datasets\n",
    "    train_dataset = get_tf_datasets(train_spectra, train_noise, std_train_aux_data, std_train_targets, \n",
    "                                    train_repeat_count, batch_size)\n",
    "    val_dataset = get_tf_datasets(val_spectra, val_noise, std_val_aux_data, std_val_targets, \n",
    "                                    val_repeat_count, batch_size)\n",
    "    \n",
    "    # get and compile model\n",
    "    model = get_model(loss, num_spectra_features, n_feature_cols, num_added_features, num_targets)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr), loss='mse')\n",
    "    \n",
    "    # fit model\n",
    "    weights_file = f'./weights/stage_1_mse_fold_{fold}_of_{num_splits}'\n",
    "    print(f'...saving checkpoint to {weights_file}')\n",
    "    callbacks = fetch_callbacks(epochs, batch_size, weights_file, patience=3)\n",
    "    history = model.fit(train_dataset, \n",
    "                          validation_data=val_dataset,\n",
    "                          epochs=epochs, \n",
    "                          shuffle=True,\n",
    "                          callbacks=callbacks)\n",
    "    \n",
    "    # append history\n",
    "    df_history = pd.DataFrame(history.history)\n",
    "    df_history['fold'] = fold\n",
    "    \n",
    "    print('...finished training model with min val_loss: {}'.format(df_history['val_loss'].min()))\n",
    "    df_full_history = pd.concat([df_full_history, df_history]).reset_index(drop=True)\n",
    "    print('...finished fold: {}, after {} seconds'.format(fold, time.time() - fold_start_time))\n",
    "    \n",
    "    fold += 1\n",
    "\n",
    "df_full_history.to_csv('histories/stage_1_mse_training_history.csv', index=False)\n",
    "print(f'finished after {time.time() - start_time} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e899486a",
   "metadata": {},
   "source": [
    "# Kfold training - phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca467f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 'uniform_quantiles'\n",
    "batch_size = 1024\n",
    "train_repeat_count = 30\n",
    "val_repeat_count = 5\n",
    "epochs = 100 # too long?\n",
    "num_splits = 5\n",
    "quantile_percents = [0.05, 0.5, 0.95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a54cef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
    "fold = 0\n",
    "second_scalers = []\n",
    "\n",
    "df_full_history = pd.DataFrame()\n",
    "start_time = time.time()\n",
    "for _train_idx, _val_idx in kf.split(train.loc[train['has_trace_data']]):\n",
    "    train_idx = train.loc[train['has_trace_data']].iloc[_train_idx].index\n",
    "    val_idx = train.loc[train['has_trace_data']].iloc[_val_idx].index\n",
    "\n",
    "    fold_start_time = time.time()\n",
    "    print('starting fold: {}'.format(fold))\n",
    "    \n",
    "    # get data\n",
    "    train_aux_data = aux_data[aux_columns].iloc[train_idx]\n",
    "    train_targets = soft_d_has_trace.iloc[train_idx]\n",
    "    train_spectra = spectra[train_idx]\n",
    "    train_noise = noise[train_idx]\n",
    "    \n",
    "    val_aux_data = aux_data[aux_columns].iloc[val_idx]\n",
    "    val_targets = soft_label_data.iloc[val_idx]\n",
    "    val_spectra = spectra[val_idx]\n",
    "    val_noise = noise[val_idx]\n",
    "    \n",
    "    # standardize aux_data and targets\n",
    "    aux_data_ss = StandardScaler()\n",
    "    std_train_aux_data = aux_data_ss.fit_transform(train_aux_data.drop(['planet_ID'], axis=1).values)\n",
    "    std_val_aux_data = aux_data_ss.transform(val_aux_data.drop(['planet_ID'], axis=1).values)\n",
    "    second_scalers.append(aux_data_ss)\n",
    "    with open(f'saved_objects/uniform_quantile_fold_{fold}_of_{num_splits}.pickle', 'wb') as f:\n",
    "        pickle.dump(aux_data_ss, f)\n",
    "    \n",
    "    # get datasets\n",
    "    train_dataset = get_tf_dataset_sampled(train_spectra, train_noise, std_train_aux_data, \n",
    "                                           train_targets['planet_ID'].values, train_repeat_count, \n",
    "                                           batch_size, trace_file, target_scales_arr, \n",
    "                                           target_labels=target_labels, split_outputs=True)\n",
    "    val_dataset = get_tf_dataset_sampled(val_spectra, val_noise, std_val_aux_data, \n",
    "                                         val_targets['planet_ID'].values, val_repeat_count, \n",
    "                                         batch_size, trace_file, target_scales_arr, \n",
    "                                         target_labels=target_labels, split_outputs=True)\n",
    "    \n",
    "    # get model with prior stage weights\n",
    "    weights_file = f'./weights/stage_1_mse_fold_{fold}_of_{num_splits}'\n",
    "    print(f'...loading model from prior stage checkpoint: {weights_file}')\n",
    "    prior_model = get_model('mse', num_spectra_features, n_feature_cols, num_added_features, num_targets, \n",
    "                            weights_file=weights_file)\n",
    "    last_layer = prior_model.get_layer('final_dropout').output\n",
    "    outputs = []\n",
    "    for i in range(len(target_labels)):\n",
    "        outputs.append(Dense(len(quantile_percents), activation=None, \n",
    "                             name='output_{}'.format(target_labels[i]))(last_layer))\n",
    "    model = Model(inputs=prior_model.inputs, outputs=outputs)\n",
    "\n",
    "    N = train_repeat_count*len(train_idx)\n",
    "    iterations = N/batch_size\n",
    "    step_size= 2 * iterations\n",
    "    lr_schedule = CyclicalLearningRate(5e-5, 1e-3, step_size=step_size, scale_fn=lambda x: tf.pow(0.95, x))\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "    \n",
    "    ckpt_weights_file = f'./weights/stage_2_uniform_quantiles_fold_{fold}_of_{num_splits}'\n",
    "    print(f'...saving model checkpoints at {ckpt_weights_file}')\n",
    "    callbacks = fetch_callbacks(epochs, batch_size, ckpt_weights_file, patience=4, ignore_lr_sched=True)\n",
    "    \n",
    "    # compile and fit\n",
    "    loss_dict = {'output_{}'.format(t): quantile_loss_wrapper(quantile_percents) for t in target_labels}\n",
    "    model.compile(optimizer=opt, loss=loss_dict)\n",
    "    history = model.fit(train_dataset, \n",
    "                      validation_data=val_dataset,\n",
    "                      epochs=epochs, \n",
    "                      shuffle=True,\n",
    "                      callbacks=callbacks)\n",
    "    \n",
    "    # append history\n",
    "    df_history = pd.DataFrame(history.history)\n",
    "    df_history['fold'] = fold\n",
    "    \n",
    "    df_full_history = pd.concat([df_full_history, df_history]).reset_index(drop=True)\n",
    "    print('...finished fold: {}, after {} seconds'.format(fold, time.time() - fold_start_time))\n",
    "    \n",
    "    fold += 1\n",
    "    \n",
    "    del(model)\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "df_full_history.to_csv('histories/stage_2_uniform_quantile_history.csv', index=False)\n",
    "print(f'finished after {time.time() - start_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9457c6f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
